from langchain_openai import ChatOpenAI
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from retrieval import load_FAISS_retriever
from langchain_community.callbacks import get_openai_callback


######################################## PROMPT ########################################
def getRagPrompt():
    return PromptTemplate.from_template("""
    You are a helpful assistant specialized in Oracle documentation.

    Answer the user‚Äôs question as clearly and accurately as possible, using the available Oracle documentation you‚Äôve been provided.
        ‚Ä¢	If the answer can be reasonably inferred from the information you have, include it.
        ‚Ä¢	If you can only answer part of the question, explain which part you can answer and provide that information.
        ‚Ä¢	If no relevant information is available, reply with:
    ‚ÄúI‚Äôm sorry, I don‚Äôt have enough information to answer that right now.‚Äù

    Make sure your response:
        ‚Ä¢	Uses clear, professional language
        ‚Ä¢	Focuses on Oracle-specific terms or behavior when relevant
        ‚Ä¢	Quotes directly from documentation when appropriate
        ‚Ä¢	Avoids mentioning how the answer was found or processed
    
    Context:
    {context}

    Question: {input}                              
    """)

def generate(question, retriever):
    llm = ChatOpenAI(model="gpt-4.1-nano")

    prompt = getRagPrompt()

    print("\n\n ## RAG GENERATION LAYER: Question received:", question,"\n")

    try:

        # This handles injecting context into prompt
        stuff_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)

        rag_chain = create_retrieval_chain(retriever, stuff_chain)

        response = rag_chain.invoke({"input": question})

        print("\n\n ######## RAG Response ###### \n\n", response.get("answer"), "\n\n")

        return {
            "status": "success",
            "answer": response.get("answer", ""),
            "context": response.get("context", []),
            "error": None
        }

    except Exception as e:
        print(f"‚ùå RAG pipeline error: {str(e)}")
        return {
            "status": "error",
            "answer": None,
            "context": [],
            "error": str(e)
        }


## Helper function to get the token cost for the LLM call
def helper_getCost(rag_chain, question):
    # Measure token usage and cost
    with get_openai_callback() as cb:

        response = rag_chain.invoke({"input": question})
        # print("üß† Answer:", response["answer"])
        
        print("/n $$$$$$$$$$$$Prompt Cost $$$$$$$$$$$$")
        print("üî¢ Prompt tokens:", cb.prompt_tokens)
        print("üìù Completion tokens:", cb.completion_tokens)
        print("üì¶ Total tokens:", cb.total_tokens)
        print("üí∞ Cost (USD):", cb.total_cost)
        print("/n $$$$$$$$$$$$Prompt Cost $$$$$$$$$$$$")
    return response



# ## Test code if running this python file
# if __name__ == "__main__":

#     print("\n processing ... \n")

#     faiss_retriever = load_FAISS_retriever().as_retriever()

#     question = "what is the difference between a policy and a purhcase order?"

#     print("\n generating response for question:",question," ... \n")  

#     response = generate(question, faiss_retriever)

#     print("\nüß† #################   Answer   ###################\n\n", response['answer'], "\n")

#     print("\n\n ####################### Context (Chunks retrieved) ####################### \n\n")

#     for i in response['context']:
#         print(i)
#         print("\n\n")